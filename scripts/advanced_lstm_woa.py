import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K
import time

# ======== 1. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u ========
np.random.seed(42)
tf.random.set_seed(42)

def preprocess_for_lstm(df):
    """Tiá»n xá»­ lÃ½ dá»¯ liá»‡u vá»›i cyclic encoding"""
    if "DATE" in df.columns:
        df = df.drop(columns=["DATE"])

    data = df.copy()

    # Cyclic encoding cho cÃ¡c biáº¿n thá»i gian
    data["HOUR_sin"] = np.sin(2 * np.pi * data["HOUR"] / 24)
    data["HOUR_cos"] = np.cos(2 * np.pi * data["HOUR"] / 24)
    data["DAY_sin"] = np.sin(2 * np.pi * data["DAY"] / 31)
    data["DAY_cos"] = np.cos(2 * np.pi * data["DAY"] / 31)
    data["MONTH_sin"] = np.sin(2 * np.pi * data["MONTH"] / 12)
    data["MONTH_cos"] = np.cos(2 * np.pi * data["MONTH"] / 12)
    data["WEEKDAY_sin"] = np.sin(2 * np.pi * data["WEEKDAY"] / 7)
    data["WEEKDAY_cos"] = np.cos(2 * np.pi * data["WEEKDAY"] / 7)
    data = data.drop(columns=["DAY", "MONTH", "HOUR", "WEEKDAY"])

    target_col = "ENERGY"
    y = data[[target_col]].values
    X = data.drop(columns=[target_col]).values

    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)

    return X_scaled, y_scaled, scaler_X, scaler_y

def create_sequences(X, y, timesteps=24):
    """Táº¡o sequences cho LSTM"""
    Xs, ys = [], []
    for i in range(len(X) - timesteps):
        Xs.append(X[i:i+timesteps])
        ys.append(y[i+timesteps])
    return np.array(Xs), np.array(ys)

# ======== 2. Äá»c dá»¯ liá»‡u ========
print("ğŸ”„ Äang táº£i dá»¯ liá»‡u...")
df = pd.read_csv("data/dataset_clean.csv")
X_scaled, y_scaled, scaler_X, scaler_y = preprocess_for_lstm(df)

timesteps = 24
X_seq, y_seq = create_sequences(X_scaled, y_scaled, timesteps)

# Chia dá»¯ liá»‡u
total_size = len(X_seq)
train_size = int(total_size * 0.7)
val_size = int(total_size * 0.15)

X_train = X_seq[:train_size]
y_train = y_seq[:train_size]
X_val = X_seq[train_size:train_size+val_size]
y_val = y_seq[train_size:train_size+val_size]
X_test = X_seq[train_size+val_size:]
y_test = y_seq[train_size+val_size:]

print(f"âœ… Dá»¯ liá»‡u: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}")

# ======== 3. HÃ m táº¡o mÃ´ hÃ¬nh nÃ¢ng cao ========
def create_advanced_lstm_model(params, input_shape):
    """
    Táº¡o mÃ´ hÃ¬nh LSTM nÃ¢ng cao vá»›i nhiá»u layer vÃ  Bidirectional
    params: [lstm_units_1, lstm_units_2, dropout_1, dropout_2, batch_norm, bidirectional, lr]
    """
    lstm_units_1 = int(params[0])
    lstm_units_2 = int(params[1])
    dropout_1 = params[2]
    dropout_2 = params[3]
    batch_norm = params[4] > 0.5  # Boolean
    bidirectional = params[5] > 0.5  # Boolean
    lr = params[6]
    
    model = Sequential()
    
    # Layer 1: LSTM hoáº·c Bidirectional LSTM
    if bidirectional:
        model.add(Bidirectional(LSTM(lstm_units_1, return_sequences=True), input_shape=input_shape))
    else:
        model.add(LSTM(lstm_units_1, return_sequences=True, input_shape=input_shape))
    
    if batch_norm:
        model.add(BatchNormalization())
    model.add(Dropout(dropout_1))
    
    # Layer 2: LSTM thá»© hai
    if bidirectional:
        model.add(Bidirectional(LSTM(lstm_units_2, return_sequences=False)))
    else:
        model.add(LSTM(lstm_units_2, return_sequences=False))
    
    if batch_norm:
        model.add(BatchNormalization())
    model.add(Dropout(dropout_2))
    
    # Output layer
    model.add(Dense(1))
    
    # Compile
    model.compile(optimizer=Adam(learning_rate=lr), loss="mse")
    
    return model

# ======== 4. HÃ m fitness tá»‘i Æ°u hÃ³a ========
def fitness_optimized(params):
    """
    HÃ m fitness tá»‘i Æ°u hÃ³a vá»›i Ã­t dá»¯ liá»‡u Ä‘á»ƒ giáº£m thá»i gian
    Chá»‰ sá»­ dá»¥ng 20% dá»¯ liá»‡u train vÃ  5 epochs
    """
    # Láº¥y subset nhá» Ä‘á»ƒ training nhanh
    subset_size = int(len(X_train) * 0.2)  # Chá»‰ 20% dá»¯ liá»‡u
    X_train_subset = X_train[:subset_size]
    y_train_subset = y_train[:subset_size]
    
    subset_val_size = int(len(X_val) * 0.2)  # Chá»‰ 20% validation
    X_val_subset = X_val[:subset_val_size]
    y_val_subset = y_val[:subset_val_size]
    
    try:
        K.clear_session()
        model = create_advanced_lstm_model(params, (X_train.shape[1], X_train.shape[2]))
        
        # Early stopping vá»›i patience tháº¥p
        es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
        
        # Chá»‰ train 5 epochs Ä‘á»ƒ nhanh
        history = model.fit(
            X_train_subset, y_train_subset,
            validation_data=(X_val_subset, y_val_subset),
            epochs=5,  # Ráº¥t Ã­t epochs
            batch_size=32,
            verbose=0,
            callbacks=[es]
        )
        
        # Tráº£ vá» validation loss cuá»‘i cÃ¹ng
        return min(history.history['val_loss'])
        
    except Exception as e:
        print(f"âŒ Lá»—i trong fitness: {e}")
        return float('inf')

# ======== 5. Thuáº­t toÃ¡n WOA cáº£i tiáº¿n ========
def WOA_improved(fitness, bounds, n_agents=10, max_iter=20):
    """
    WOA cáº£i tiáº¿n vá»›i nhiá»u agents vÃ  iterations hÆ¡n
    """
    dim = len(bounds)
    lb = np.array([b[0] for b in bounds])
    ub = np.array([b[1] for b in bounds])

    # Khá»Ÿi táº¡o whales
    whales = np.random.rand(n_agents, dim) * (ub - lb) + lb
    best_pos = whales[0].copy()
    best_score = fitness(best_pos)
    
    print(f"ğŸ¯ Báº¯t Ä‘áº§u WOA vá»›i {n_agents} agents, {max_iter} iterations")
    print(f"â±ï¸  Æ¯á»›c tÃ­nh thá»i gian: ~{n_agents * max_iter * 5 * 0.2 * 0.1:.1f} phÃºt")

    for i in range(max_iter):
        a = 2 - i * (2 / max_iter)  # giáº£m dáº§n tá»« 2 xuá»‘ng 0
        
        for j in range(n_agents):
            r = np.random.rand(dim)
            A = 2 * a * r - a
            C = 2 * r
            p = np.random.rand()
            b = 1
            l = np.random.uniform(-1, 1, dim)

            if p < 0.5:
                if np.linalg.norm(A) < 1:
                    # Exploitation: tÃ¬m kiáº¿m xung quanh best
                    D = abs(C * best_pos - whales[j])
                    whales[j] = best_pos - A * D
                else:
                    # Exploration: tÃ¬m kiáº¿m ngáº«u nhiÃªn
                    rand_pos = whales[np.random.randint(n_agents)]
                    D = abs(C * rand_pos - whales[j])
                    whales[j] = rand_pos - A * D
            else:
                # Spiral updating
                D = abs(best_pos - whales[j])
                whales[j] = D * np.exp(b * l) * np.cos(2 * np.pi * l) + best_pos

            # Giá»›i háº¡n trong bounds
            whales[j] = np.clip(whales[j], lb, ub)
            
            # ÄÃ¡nh giÃ¡ fitness
            score = fitness(whales[j])
            if score < best_score:
                best_score = score
                best_pos = whales[j].copy()

        print(f"Iter {i+1}/{max_iter} - Best val_loss: {best_score:.6f}")

    return best_pos, best_score

# ======== 6. Cháº¡y tá»‘i Æ°u hÃ³a ========
print("\nğŸš€ Báº®T Äáº¦U Tá»I Æ¯U HÃ“A MÃ” HÃŒNH NÃ‚NG CAO")
print("=" * 60)

# Bounds cho cÃ¡c tham sá»‘
bounds = [
    (32, 256),      # lstm_units_1
    (16, 128),      # lstm_units_2  
    (0.1, 0.6),     # dropout_1
    (0.1, 0.6),     # dropout_2
    (0, 1),         # batch_norm (0 hoáº·c 1)
    (0, 1),         # bidirectional (0 hoáº·c 1)
    (1e-4, 1e-2)    # learning_rate
]

print("ğŸ“‹ Tham sá»‘ tá»‘i Æ°u hÃ³a:")
print("   â€¢ LSTM units 1: 32-256")
print("   â€¢ LSTM units 2: 16-128")
print("   â€¢ Dropout 1: 0.1-0.6")
print("   â€¢ Dropout 2: 0.1-0.6")
print("   â€¢ Batch Normalization: CÃ³/KhÃ´ng")
print("   â€¢ Bidirectional: CÃ³/KhÃ´ng")
print("   â€¢ Learning Rate: 1e-4 Ä‘áº¿n 1e-2")

start_time = time.time()
best_params, best_score = WOA_improved(fitness_optimized, bounds, n_agents=10, max_iter=20)
optimization_time = time.time() - start_time

print(f"\nâœ… HOÃ€N THÃ€NH Tá»I Æ¯U HÃ“A trong {optimization_time:.1f} giÃ¢y")
print("=" * 50)
print("ğŸ¯ Káº¾T QUáº¢ Tá»I Æ¯U:")
print(f"   â€¢ Best val_loss: {best_score:.6f}")
print(f"   â€¢ LSTM units 1: {int(best_params[0])}")
print(f"   â€¢ LSTM units 2: {int(best_params[1])}")
print(f"   â€¢ Dropout 1: {best_params[2]:.3f}")
print(f"   â€¢ Dropout 2: {best_params[3]:.3f}")
print(f"   â€¢ Batch Norm: {'CÃ³' if best_params[4] > 0.5 else 'KhÃ´ng'}")
print(f"   â€¢ Bidirectional: {'CÃ³' if best_params[5] > 0.5 else 'KhÃ´ng'}")
print(f"   â€¢ Learning Rate: {best_params[6]:.6f}")

# ======== 7. Train mÃ´ hÃ¬nh cuá»‘i cÃ¹ng ========
print(f"\nğŸ‹ï¸  TRAIN MÃ” HÃŒNH CUá»I CÃ™NG")
print("=" * 40)

final_model = create_advanced_lstm_model(best_params, (X_train.shape[1], X_train.shape[2]))
es_final = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

print("ğŸ“Š Kiáº¿n trÃºc mÃ´ hÃ¬nh cuá»‘i cÃ¹ng:")
final_model.summary()

# Train vá»›i toÃ n bá»™ dá»¯ liá»‡u
train_start = time.time()
history = final_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    verbose=1,
    callbacks=[es_final]
)
train_time = time.time() - train_start

print(f"âœ… HoÃ n thÃ nh training trong {train_time:.1f} giÃ¢y")

# LÆ°u mÃ´ hÃ¬nh
final_model.save("models/advanced_lstm_model.h5")
print("ğŸ’¾ MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: models/advanced_lstm_model.h5")

# ======== 8. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh ========
print(f"\nğŸ“ˆ ÄÃNH GIÃ MÃ” HÃŒNH")
print("=" * 30)

y_pred_scaled = final_model.predict(X_test)
y_pred = scaler_y.inverse_transform(y_pred_scaled)
y_test_orig = scaler_y.inverse_transform(y_test)

# TÃ­nh metrics
mae = mean_absolute_error(y_test_orig, y_pred)
mse = mean_squared_error(y_test_orig, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_orig, y_pred)
mape = np.mean(np.abs((y_test_orig - y_pred) / y_test_orig)) * 100

print(f"ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ:")
print(f"   â€¢ MAE: {mae:.4f}")
print(f"   â€¢ MSE: {mse:.4f}")
print(f"   â€¢ RMSE: {rmse:.4f}")
print(f"   â€¢ RÂ²: {r2:.4f}")
print(f"   â€¢ MAPE: {mape:.2f}%")

# Váº½ Ä‘á»“ thá»‹
plt.figure(figsize=(15, 10))

# Subplot 1: So sÃ¡nh dá»± Ä‘oÃ¡n
plt.subplot(2, 2, 1)
plt.plot(y_test_orig[:200], label='Thá»±c táº¿', linewidth=2)
plt.plot(y_pred[:200], label='Dá»± Ä‘oÃ¡n', linewidth=2)
plt.title('So sÃ¡nh Dá»± Ä‘oÃ¡n vs Thá»±c táº¿ (200 Ä‘iá»ƒm Ä‘áº§u)')
plt.xlabel('Thá»i Ä‘iá»ƒm')
plt.ylabel('NÄƒng lÆ°á»£ng tiÃªu thá»¥')
plt.legend()
plt.grid(True, alpha=0.3)

# Subplot 2: Scatter plot
plt.subplot(2, 2, 2)
plt.scatter(y_test_orig, y_pred, alpha=0.6)
plt.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)
plt.xlabel('GiÃ¡ trá»‹ thá»±c táº¿')
plt.ylabel('GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n')
plt.title('Scatter Plot: Thá»±c táº¿ vs Dá»± Ä‘oÃ¡n')
plt.grid(True, alpha=0.3)

# Subplot 3: Training history
plt.subplot(2, 2, 3)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training History')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Subplot 4: Error distribution
plt.subplot(2, 2, 4)
errors = y_test_orig.flatten() - y_pred.flatten()
plt.hist(errors, bins=50, alpha=0.7, edgecolor='black')
plt.title('PhÃ¢n bá»‘ Sai sá»‘ Dá»± Ä‘oÃ¡n')
plt.xlabel('Sai sá»‘')
plt.ylabel('Táº§n suáº¥t')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# LÆ°u káº¿t quáº£
results_df = pd.DataFrame({
    'ThucTe': y_test_orig.flatten(),
    'DuDoan': y_pred.flatten()
})
results_df.to_csv("data/advanced_lstm_predictions.csv", index=False)

# LÆ°u metrics
metrics_df = pd.DataFrame({
    'Metric': ['MAE', 'MSE', 'RMSE', 'RÂ²', 'MAPE'],
    'Value': [mae, mse, rmse, r2, mape]
})
metrics_df.to_csv("data/advanced_lstm_metrics.csv", index=False)

print(f"\nğŸ‰ HOÃ€N THÃ€NH!")
print("=" * 20)
print(f"â±ï¸  Tá»•ng thá»i gian: {optimization_time + train_time:.1f} giÃ¢y")
print(f"ğŸ“ Files Ä‘Ã£ táº¡o:")
print(f"   â€¢ models/advanced_lstm_model.h5")
print(f"   â€¢ data/advanced_lstm_predictions.csv")
print(f"   â€¢ data/advanced_lstm_metrics.csv")
