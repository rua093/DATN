#!/usr/bin/env python3
"""
Script so s√°nh c√°c m√¥ h√¨nh LSTM kh√°c nhau
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import load_model
import warnings
import os
warnings.filterwarnings('ignore')

def load_and_compare_models():
    """So s√°nh t·∫•t c·∫£ c√°c m√¥ h√¨nh ƒë√£ train"""
    
    print("üîÑ ƒêang t·∫£i v√† so s√°nh c√°c m√¥ h√¨nh...")
    
    # ƒê·ªçc d·ªØ li·ªáu
    df = pd.read_csv("data/dataset_clean.csv")
    
    # Ti·ªÅn x·ª≠ l√Ω (gi·ªëng nh∆∞ trong training)
    if "DATE" in df.columns:
        df = df.drop(columns=["DATE"])
    
    data = df.copy()
    data["DAY_sin"] = np.sin(2 * np.pi * data["DAY"] / 31)
    data["DAY_cos"] = np.cos(2 * np.pi * data["DAY"] / 31)
    data["MONTH_sin"] = np.sin(2 * np.pi * data["MONTH"] / 12)
    data["MONTH_cos"] = np.cos(2 * np.pi * data["MONTH"] / 12)
    data["WEEKDAY_sin"] = np.sin(2 * np.pi * data["WEEKDAY"] / 7)
    data["WEEKDAY_cos"] = np.cos(2 * np.pi * data["WEEKDAY"] / 7)
    data = data.drop(columns=["DAY", "MONTH", "WEEKDAY"])
    
    target_col = "ENERGY_ADJ" if "ENERGY_ADJ" in data.columns else "ENERGY"
    y = data[[target_col]].values
    X = data.drop(columns=[target_col]).values
    
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    # T·∫°o sequences
    def create_sequences(X, y, timesteps=24):
        Xs, ys = [], []
        for i in range(len(X) - timesteps):
            Xs.append(X[i:i+timesteps])
            ys.append(y[i+timesteps])
        return np.array(Xs), np.array(ys)
    
    X_seq, y_seq = create_sequences(X_scaled, y_scaled, 24)
    
    # Chia d·ªØ li·ªáu test
    total_size = len(X_seq)
    train_size = int(total_size * 0.7)
    val_size = int(total_size * 0.15)
    
    X_test = X_seq[train_size+val_size:]
    y_test = y_seq[train_size+val_size:]
    y_test_orig = scaler_y.inverse_transform(y_test)
    
    print(f"‚úÖ D·ªØ li·ªáu test: {X_test.shape[0]} m·∫´u")
    
    # Danh s√°ch c√°c m√¥ h√¨nh ƒë·ªÉ so s√°nh
    models_to_compare = [
        {
            'name': 'LSTM C∆° b·∫£n',
            'path': 'models/my_lstm_model_optimized.h5',
            'color': 'blue'
        },
        {
            'name': 'LSTM + WOA',
            'path': 'models/my_lstm_model_woa.h5',
            'color': 'orange'
        },
        {
            'name': 'Advanced LSTM',
            'path': 'models/advanced_lstm_model.h5',
            'color': 'green'
        }
    ]
    
    results = []
    
    for model_info in models_to_compare:
        try:
            print(f"üîÑ ƒêang ƒë√°nh gi√° {model_info['name']}...")
            
            # Load model
            model = load_model(model_info['path'], compile=False)
            
            # D·ª± ƒëo√°n
            y_pred_scaled = model.predict(X_test)
            y_pred = scaler_y.inverse_transform(y_pred_scaled)
            
            # T√≠nh metrics
            mae = mean_absolute_error(y_test_orig, y_pred)
            mse = mean_squared_error(y_test_orig, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_test_orig, y_pred)
            mape = np.mean(np.abs((y_test_orig - y_pred) / y_test_orig)) * 100
            
            results.append({
                'Model': model_info['name'],
                'MAE': mae,
                'MSE': mse,
                'RMSE': rmse,
                'R¬≤': r2,
                'MAPE': mape,
                'Color': model_info['color'],
                'Predictions': y_pred
            })
            
            print(f"   ‚úÖ MAE: {mae:.4f}, R¬≤: {r2:.4f}")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói: {e}")
    
    return results, y_test_orig

def create_comparison_plots(results, y_test_orig):
    """T·∫°o bi·ªÉu ƒë·ªì so s√°nh"""
    
    print("üìä ƒêang t·∫°o bi·ªÉu ƒë·ªì so s√°nh...")
    
    # T·∫°o DataFrame cho k·∫øt qu·∫£
    df_results = pd.DataFrame([{
        'Model': r['Model'],
        'MAE': r['MAE'],
        'MSE': r['MSE'],
        'RMSE': r['RMSE'],
        'R¬≤': r['R¬≤'],
        'MAPE': r['MAPE']
    } for r in results])
    
    # T·∫°o subplot
    os.makedirs("../results", exist_ok=True)
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('So s√°nh Hi·ªáu su·∫•t c√°c M√¥ h√¨nh LSTM', fontsize=16, fontweight='bold')
    
    # 1. Bar chart metrics
    metrics = ['MAE', 'RMSE', 'R¬≤', 'MAPE']
    colors = ['blue', 'orange', 'green']
    
    for i, metric in enumerate(metrics):
        ax = axes[0, i] if i < 2 else axes[1, i-2]
        
        bars = ax.bar(df_results['Model'], df_results[metric], 
                     color=colors[:len(df_results)], alpha=0.7)
        ax.set_title(f'{metric} Comparison')
        ax.set_ylabel(metric)
        ax.tick_params(axis='x', rotation=45)
        
        # Th√™m gi√° tr·ªã l√™n bars
        for bar, value in zip(bars, df_results[metric]):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{value:.3f}', ha='center', va='bottom')
    
    # 2. Prediction comparison (200 ƒëi·ªÉm ƒë·∫ßu)
    ax = axes[1, 2]
    ax.plot(y_test_orig[:200], label='Th·ª±c t·∫ø', linewidth=2, color='black')
    
    for i, result in enumerate(results):
        ax.plot(result['Predictions'][:200], 
               label=result['Model'], 
               linewidth=1.5, 
               color=result['Color'],
               alpha=0.8)
    
    ax.set_title('So s√°nh D·ª± ƒëo√°n (200 ƒëi·ªÉm ƒë·∫ßu)')
    ax.set_xlabel('Th·ªùi ƒëi·ªÉm')
    ax.set_ylabel('NƒÉng l∆∞·ª£ng ti√™u th·ª•')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig("results/compare_metrics_and_predictions.png", dpi=200)
    plt.close()
    
    # 3. Scatter plots cho t·ª´ng m√¥ h√¨nh
    fig, axes = plt.subplots(1, len(results), figsize=(5*len(results), 5))
    if len(results) == 1:
        axes = [axes]
    
    for i, result in enumerate(results):
        ax = axes[i]
        ax.scatter(y_test_orig, result['Predictions'], alpha=0.6, color=result['Color'])
        ax.plot([y_test_orig.min(), y_test_orig.max()], 
               [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)
        ax.set_xlabel('Gi√° tr·ªã th·ª±c t·∫ø')
        ax.set_ylabel('Gi√° tr·ªã d·ª± ƒëo√°n')
        ax.set_title(f'{result["Model"]}\nR¬≤ = {result["R¬≤"]:.3f}')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig("results/compare_scatter_each_model.png", dpi=200)
    plt.close()

def print_detailed_comparison(results):
    """In b√°o c√°o so s√°nh chi ti·∫øt"""
    
    print("\n" + "="*80)
    print("                    B√ÅO C√ÅO SO S√ÅNH M√î H√åNH CHI TI·∫æT")
    print("="*80)
    
    # T·∫°o b·∫£ng so s√°nh
    df_results = pd.DataFrame([{
        'Model': r['Model'],
        'MAE': r['MAE'],
        'MSE': r['MSE'],
        'RMSE': r['RMSE'],
        'R¬≤': r['R¬≤'],
        'MAPE': r['MAPE']
    } for r in results])
    
    print("\nüìä B·∫¢NG SO S√ÅNH METRICS:")
    print("-" * 80)
    print(df_results.to_string(index=False, float_format='%.4f'))
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t cho t·ª´ng metric
    print(f"\nüèÜ M√î H√åNH T·ªêT NH·∫§T CHO T·ª™NG METRIC:")
    print("-" * 50)
    
    for metric in ['MAE', 'MSE', 'RMSE', 'MAPE']:
        if metric == 'R¬≤':
            best_idx = df_results[metric].idxmax()
        else:
            best_idx = df_results[metric].idxmin()
        best_model = df_results.loc[best_idx, 'Model']
        best_value = df_results.loc[best_idx, metric]
        print(f"   ‚Ä¢ {metric}: {best_model} ({best_value:.4f})")
    
    # Ph√¢n t√≠ch t·ªïng quan
    print(f"\nüìà PH√ÇN T√çCH T·ªîNG QUAN:")
    print("-" * 30)
    
    # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p (lower is better cho MAE, MSE, RMSE, MAPE; higher is better cho R¬≤)
    df_results['Score'] = (
        -df_results['MAE']/df_results['MAE'].max() +  # Normalize v√† ƒë·∫£o ng∆∞·ª£c
        -df_results['MSE']/df_results['MSE'].max() +
        -df_results['RMSE']/df_results['RMSE'].max() +
        -df_results['MAPE']/df_results['MAPE'].max() +
        df_results['R¬≤']/df_results['R¬≤'].max()
    )
    
    best_overall_idx = df_results['Score'].idxmax()
    best_overall = df_results.loc[best_overall_idx, 'Model']
    
    print(f"   ‚Ä¢ M√¥ h√¨nh t·ªïng th·ªÉ t·ªët nh·∫•t: {best_overall}")
    print(f"   ‚Ä¢ ƒêi·ªÉm t·ªïng h·ª£p: {df_results.loc[best_overall_idx, 'Score']:.3f}")
    
    # Khuy·∫øn ngh·ªã
    print(f"\nüí° KHUY·∫æN NGH·ªä:")
    print("-" * 20)
    print(f"   ‚Ä¢ S·ª≠ d·ª•ng {best_overall} cho d·ª± √°n")
    print(f"   ‚Ä¢ C√¢n nh·∫Øc ensemble n·∫øu c·∫ßn ƒë·ªô ch√≠nh x√°c cao")
    print(f"   ‚Ä¢ Th·ª≠ th√™m features n·∫øu mu·ªën c·∫£i thi·ªán h∆°n")

def save_comparison_results(results):
    """L∆∞u k·∫øt qu·∫£ so s√°nh"""
    
    # T·∫°o DataFrame
    df_results = pd.DataFrame([{
        'Model': r['Model'],
        'MAE': r['MAE'],
        'MSE': r['MSE'],
        'RMSE': r['RMSE'],
        'R¬≤': r['R¬≤'],
        'MAPE': r['MAPE']
    } for r in results])
    
    # L∆∞u CSV
    df_results.to_csv("data/model_comparison_results.csv", index=False)
    print("üíæ K·∫øt qu·∫£ ƒë√£ l∆∞u: data/model_comparison_results.csv")
    
    # L∆∞u JSON cho web
    import json
    comparison_data = {
        'timestamp': pd.Timestamp.now().isoformat(),
        'models': df_results.to_dict('records')
    }
    
    with open("data/model_comparison_results.json", "w", encoding="utf-8") as f:
        json.dump(comparison_data, f, indent=2, ensure_ascii=False)
    
    print("üíæ JSON ƒë√£ l∆∞u: data/model_comparison_results.json")

def main():
    """H√†m ch√≠nh"""
    print("üîÑ B·∫ÆT ƒê·∫¶U SO S√ÅNH M√î H√åNH")
    print("=" * 50)
    
    # Load v√† so s√°nh m√¥ h√¨nh
    results, y_test_orig = load_and_compare_models()
    
    if not results:
        print("‚ùå Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë·ªÉ so s√°nh")
        return
    
    # T·∫°o bi·ªÉu ƒë·ªì
    create_comparison_plots(results, y_test_orig)
    
    # In b√°o c√°o chi ti·∫øt
    print_detailed_comparison(results)
    
    # L∆∞u k·∫øt qu·∫£
    save_comparison_results(results)
    
    print(f"\nüéâ HO√ÄN TH√ÄNH SO S√ÅNH!")
    print("=" * 30)
    print(f"üìä ƒê√£ so s√°nh {len(results)} m√¥ h√¨nh")
    print(f"üìÅ K·∫øt qu·∫£ ƒë√£ l∆∞u trong data/")

if __name__ == "__main__":
    main()
