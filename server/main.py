from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Query
from fastapi.responses import FileResponse
from sqlalchemy.orm import Session
from pydantic import BaseModel
from typing import Optional
import logging
from datetime import datetime
from dotenv import load_dotenv
load_dotenv()
from server.services.ai_services import ask_energy_ai, ask_energy_ai_for_user

from server.services.training_service import TrainingService

import os

from server.database import (
    init_db, get_db, SessionLocal, get_account_by_username, create_account,
    get_active_model, create_model, EvnAccount, TrainingJob, CrawlJob, DailyConsumption,
    update_account_password
)
from server.services.crawler_service import CrawlerService
from server.services.training_service import TrainingService
from server.config import (
    MODELS_DIR, FINE_TUNE_LR, FINE_TUNE_EPOCHS
)
from fastapi import FastAPI, Body
from server.services.ai_services import ask_energy_ai

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(title="EVN Energy Prediction API", version="1.0.0")
init_db()


def get_user_by_username(evn_username: str = Query(..., description="EVN username"), db: Session = Depends(get_db)) -> EvnAccount:
    """Get user by evn_username from query parameter"""
    user = get_account_by_username(db, evn_username)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user

def handle_first_login(evn_username: str, evn_password: str):
    logger.info(f"üöÄ handle_first_login ƒë∆∞·ª£c g·ªçi cho user: {evn_username}")
    db = SessionLocal()
    job = None
    try:
        job = TrainingJob(evn_username=evn_username, status="running")
        db.add(job)
        db.commit()
        logger.info(f"B·∫Øt ƒë·∫ßu thi·∫øt l·∫≠p l·∫ßn ƒë·∫ßu cho ng∆∞·ªùi d√πng {evn_username}")
        # T·∫°o CrawlJob v√† c·∫≠p nh·∫≠t tr·∫°ng th√°i theo ti·∫øn tr√¨nh
        crawl_job = CrawlJob(evn_username=evn_username, status="running")
        db.add(crawl_job)
        db.commit()
        crawler_service = CrawlerService(evn_username, evn_password)
        crawl_result = crawler_service.crawl_initial_data(evn_username, years_back=3)
        if not crawl_result["success"]:
            if crawl_job:
                crawl_job.status = "failed"
                crawl_job.error_message = crawl_result.get("error")
                crawl_job.completed_at = datetime.utcnow()
                db.commit()
            if job:
                job.status = "failed"
                job.error_message = f"Crawl th·∫•t b·∫°i: {crawl_result.get('error')}"
                db.commit()
            acc = get_account_by_username(db, evn_username)
            if acc:
                acc.crawl_status = "failed"
                db.commit()
            return
        else:
            if crawl_job:
                crawl_job.status = "completed"
                crawl_job.completed_at = datetime.utcnow()
                crawl_job.records_crawled = crawl_result.get("records")
                db.commit()
        training_service = TrainingService()
        train_result = training_service.train_model(
            evn_username, db,
            fine_tune_lr=FINE_TUNE_LR,
            fine_tune_epochs=FINE_TUNE_EPOCHS
        )
        if not train_result["success"]:
            if job:
                job.status = "failed"
                job.error_message = f"Train th·∫•t b·∫°i: {train_result.get('error')}"
                db.commit()
            acc = get_account_by_username(db, evn_username)
            if acc:
                acc.crawl_status = "failed"
                db.commit()
            return
        model = create_model(
            db=db, evn_username=evn_username, model_path=train_result["model_path"],
            scaler_x_path=train_result.get("scaler_x_path"), scaler_y_path=train_result.get("scaler_y_path"),
            metrics=train_result.get("metrics"), training_params=train_result.get("training_params")
        )
        u = get_account_by_username(db, evn_username)
        if u:
            u.crawl_status = "success"
            u.model_path = train_result["model_path"]
            db.commit()
        if job:
            job.status = "completed"
            job.completed_at = datetime.utcnow()
            job.model_id = model.id
            db.commit()
        logger.info(f"Thi·∫øt l·∫≠p l·∫ßn ƒë·∫ßu ho√†n t·∫•t cho ng∆∞·ªùi d√πng {evn_username}")
    except Exception as e:
        logger.error(f"L·ªói trong handle_first_login: {str(e)}", exc_info=True)
        try:
            if job:
                job.status = "failed"
                job.error_message = str(e)
                db.commit()
            # ƒë√°nh d·∫•u crawl job g·∫ßn nh·∫•t l√† failed n·∫øu c√≥
            try:
                last_crawl = db.query(CrawlJob).filter(CrawlJob.evn_username == evn_username).order_by(CrawlJob.started_at.desc()).first()
                if last_crawl and last_crawl.status == "running":
                    last_crawl.status = "failed"
                    last_crawl.error_message = str(e)
                    last_crawl.completed_at = datetime.utcnow()
                    db.commit()
            except Exception:
                pass
            acc = get_account_by_username(db, evn_username)
            if acc:
                acc.crawl_status = "failed"
                db.commit()
        except Exception as e2:
            logger.error(f"L·ªói khi c·∫≠p nh·∫≠t job status: {str(e2)}", exc_info=True)
    finally:
        try:
            db.close()
        except Exception:
            pass

@app.get("/")
async def root():
    return {"message": "EVN Energy Prediction API", "version": "1.0.0"}

# POST /api/auth/login with immediate EVN verification
class AuthLoginRequest(BaseModel):
    evn_username: str
    evn_password: str
    location: Optional[str] = None

@app.post("/api/auth/login")
async def auth_login(request: AuthLoginRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    user = get_account_by_username(db, request.evn_username)
    if not user:
        # User m·ªõi: t·∫°o account v√† crawl + train
        user = create_account(db=db, evn_username=request.evn_username, evn_password=request.evn_password, location=request.location)
        user.crawl_status = "pending"
        db.commit()
        background_tasks.add_task(handle_first_login, request.evn_username, request.evn_password)
        return {"status": "pending", "message": "X√°c th·ª±c th√†nh c√¥ng. ƒêang x·ª≠ l√Ω d·ªØ li·ªáu..."}
    
    # User ƒë√£ t·ªìn t·∫°i: c·∫≠p nh·∫≠t password v√† location
    password_changed = user.evn_password != request.evn_password
    update_account_password(db, request.evn_username, request.evn_password)
    if request.location:
        user.location = request.location
    db.commit()
    
    # Refresh user t·ª´ DB ƒë·ªÉ l·∫•y crawl_status m·ªõi nh·∫•t
    db.refresh(user)

    if password_changed:
        user.crawl_status = "pending"
        db.commit()
        background_tasks.add_task(handle_first_login, request.evn_username, request.evn_password)
        return {"status": "pending", "message": "M·∫≠t kh·∫©u ƒë√£ thay ƒë·ªïi. ƒêang x√°c th·ª±c l·∫°i d·ªØ li·ªáu..."}
    
    # Ki·ªÉm tra xem ƒë√£ c√≥ model v√† crawl_status = "success" ch∆∞a
    active_model = get_active_model(db, user.evn_username)
    crawl_status = user.crawl_status or "pending"
    
    logger.info(f"User {request.evn_username} - crawl_status: {crawl_status}, has_model: {active_model is not None}")
    
    if not active_model or crawl_status != "success":
        # Ch∆∞a c√≥ model ho·∫∑c crawl_status ch∆∞a success ‚Üí crawl l·∫°i
        logger.info(f"User {request.evn_username} ch∆∞a c√≥ model ho·∫∑c crawl_status != success, b·∫Øt ƒë·∫ßu crawl l·∫°i...")
        if crawl_status != "pending":
            user.crawl_status = "pending"
            db.commit()
        background_tasks.add_task(handle_first_login, request.evn_username, request.evn_password)
        return {"status": "pending", "message": "ƒêang crawl v√† train l·∫°i d·ªØ li·ªáu..."}
    
    # ƒê√£ c√≥ model th√†nh c√¥ng
    return {"status": "success", "message": "ƒêƒÉng nh·∫≠p th√†nh c√¥ng!"}

@app.get("/api/model/status")
async def get_model_status(user: EvnAccount = Depends(get_user_by_username), db: Session = Depends(get_db)):
    active_model = get_active_model(db, user.evn_username)
    if not active_model:
        return {"has_model": False, "message": "No active model found"}
    model_exists = os.path.exists(active_model.model_path)
    return {
        "has_model": True, "model_id": active_model.id, "trained_at": active_model.trained_at.isoformat(),
        "metrics": {"mae": active_model.metrics_mae, "rmse": active_model.metrics_rmse, "r2": active_model.metrics_r2},
        "model_exists": model_exists
    }

@app.get("/api/model/download")
async def download_model(user: EvnAccount = Depends(get_user_by_username), db: Session = Depends(get_db)):
    active_model = get_active_model(db, user.evn_username)
    if not active_model:
        raise HTTPException(status_code=404, detail="No active model found")
    if not os.path.exists(active_model.model_path):
        raise HTTPException(status_code=404, detail="Model file not found")
    return FileResponse(active_model.model_path, media_type="application/octet-stream", filename="lstm_model.h5")

@app.get("/api/model/scalers/download")
async def download_scalers(user: EvnAccount = Depends(get_user_by_username), db: Session = Depends(get_db)):
    active_model = get_active_model(db, user.evn_username)
    if not active_model:
        raise HTTPException(status_code=404, detail="No active model found")
    return {"scaler_x_path": active_model.scaler_x_path, "scaler_y_path": active_model.scaler_y_path}

@app.get("/api/training/jobs")
async def get_training_jobs(user: EvnAccount = Depends(get_user_by_username), db: Session = Depends(get_db)):
    jobs = db.query(TrainingJob).filter(TrainingJob.evn_username == user.evn_username).order_by(TrainingJob.started_at.desc()).limit(10).all()
    return [{"id": job.id, "status": job.status, "started_at": job.started_at.isoformat(),
             "completed_at": job.completed_at.isoformat() if job.completed_at else None, "error_message": job.error_message} for job in jobs]

@app.get("/api/crawl/jobs")
async def get_crawl_jobs(user: EvnAccount = Depends(get_user_by_username), db: Session = Depends(get_db)):
    jobs = db.query(CrawlJob).filter(
        CrawlJob.evn_username == user.evn_username
    ).order_by(CrawlJob.started_at.desc()).limit(10).all()
    return [
        {
            "id": job.id,
            "status": job.status,
            "crawl_date": job.crawl_date.isoformat() if job.crawl_date else None,
            "started_at": job.started_at.isoformat(),
            "completed_at": job.completed_at.isoformat() if job.completed_at else None,
            "records_crawled": job.records_crawled,
            "error_message": job.error_message
        }
        for job in jobs
    ]

@app.get("/api/data/forecast")
async def get_forecast(user: EvnAccount = Depends(get_user_by_username), db: Session = Depends(get_db)):
    import joblib
    import numpy as np
    from server.services.training_service import TrainingService
    from server.database import get_account_by_username
    model_dir = MODELS_DIR / f"user_{user.evn_username}"
    model_path = model_dir / "lstm_model.h5"
    sx_path = model_dir / "scaler_x.pkl"
    sy_path = model_dir / "scaler_y.pkl"
    if not (model_path.exists() and sx_path.exists() and sy_path.exists()):
        raise HTTPException(status_code=404, detail="Model ch∆∞a s·∫µn s√†ng")
    acc = get_account_by_username(db, user.evn_username)
    if not acc:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y t√†i kho·∫£n")
    location = acc.location if acc.location else "Ho Chi Minh City"
    import tensorflow as tf
    model = tf.keras.models.load_model(model_path, compile=False)
    timesteps = 7
    scaler = joblib.load(sx_path)
    ts = TrainingService()
    df = ts.build_dataset_from_db(db, user.evn_username, location)
    if df.empty:
        raise HTTPException(status_code=404, detail="Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o")
    df_processed = ts.preprocess_for_base_model(df)
    if len(df_processed) < timesteps + 1:
        raise HTTPException(status_code=400, detail=f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o. C·∫ßn √≠t nh·∫•t {timesteps + 1} m·∫´u")
    # L·∫•y ng√†y cu·ªëi c√πng t·ª´ d·ªØ li·ªáu g·ªëc ƒë·ªÉ t√≠nh ng√†y d·ª± b√°o
    last_date = None
    import pandas as pd
    if 'DATE_ONLY' in df.columns:
        last_date = df['DATE_ONLY'].iloc[-1]
    elif 'DATE' in df.columns:
        # Parse t·ª´ c·ªôt DATE n·∫øu DATE_ONLY kh√¥ng c√≥
        try:
            date_parsed = pd.to_datetime(df['DATE'].iloc[-1], errors='coerce', dayfirst=True)
            if pd.notna(date_parsed):
                last_date = date_parsed.date()
            else:
                logger.warning(f"Kh√¥ng parse ƒë∆∞·ª£c ng√†y t·ª´ c·ªôt DATE: {df['DATE'].iloc[-1]}")
        except Exception as e:
            logger.warning(f"L·ªói khi parse ng√†y t·ª´ c·ªôt DATE: {str(e)}")
    else:
        logger.warning(f"Kh√¥ng t√¨m th·∫•y c·ªôt DATE ho·∫∑c DATE_ONLY trong dataframe. C√°c c·ªôt c√≥ s·∫µn: {list(df.columns)}")
    if last_date:
        from datetime import timedelta
        forecast_date = last_date + timedelta(days=1)
        forecast_date_str = forecast_date.strftime('%Y-%m-%d')
    else:
        forecast_date_str = None
        logger.warning("Kh√¥ng l·∫•y ƒë∆∞·ª£c ng√†y cu·ªëi c√πng t·ª´ d·ªØ li·ªáu, forecast_date s·∫Ω l√† null")
    window = df_processed.iloc[-timesteps:].values
    window_scaled = scaler.transform(window)
    x_in = np.expand_dims(window_scaled, axis=0)
    y_hat_scaled = model.predict(x_in, verbose=0)
    n_feat = window_scaled.shape[1] - 1
    dummy = np.zeros((1, n_feat + 1))
    dummy[0, -1] = y_hat_scaled[0, 0]
    y_hat = scaler.inverse_transform(dummy)[0, -1]
    return {
        "horizon": 1, 
        "unit": "days", 
        "forecast_date": forecast_date_str,
        "predictions": [float(y_hat)]
    }

def forecast_multiple_days(
    model, scaler, df_processed, df_original, timesteps, num_days, location, db
):
    """Helper function ƒë·ªÉ d·ª± b√°o nhi·ªÅu ng√†y"""
    import pandas as pd
    import numpy as np
    from datetime import timedelta
    from server.services.training_service import TrainingService
    from scripts.preprocess import fetch_open_meteo_weather, add_holiday_and_calendar_cols
    
    # L·∫•y ng√†y cu·ªëi c√πng
    last_date = None
    if 'DATE_ONLY' in df_original.columns:
        last_date = df_original['DATE_ONLY'].iloc[-1]
    elif 'DATE' in df_original.columns:
        try:
            date_parsed = pd.to_datetime(df_original['DATE'].iloc[-1], errors='coerce', dayfirst=True)
            if pd.notna(date_parsed):
                last_date = date_parsed.date()
        except Exception:
            pass
    
    if not last_date:
        raise ValueError("Kh√¥ng l·∫•y ƒë∆∞·ª£c ng√†y cu·ªëi c√πng t·ª´ d·ªØ li·ªáu")
    
    # L·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt cho c√°c ng√†y d·ª± b√°o
    start_forecast = last_date + timedelta(days=1)
    end_forecast = last_date + timedelta(days=num_days)
    start_str = start_forecast.strftime('%Y-%m-%d')
    end_str = end_forecast.strftime('%Y-%m-%d')
    
    try:
        weather_forecast = fetch_open_meteo_weather(start_str, end_str, location=location)
    except Exception as e:
        logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu th·ªùi ti·∫øt t·ª´ API: {str(e)}, s·ª≠ d·ª•ng gi√° tr·ªã trung b√¨nh")
        # S·ª≠ d·ª•ng gi√° tr·ªã trung b√¨nh t·ª´ d·ªØ li·ªáu l·ªãch s·ª≠
        temp_avg_mean = df_original['TEMPERATURE_AVG'].mean() if 'TEMPERATURE_AVG' in df_original.columns else 28.0
        humidity_avg_mean = df_original['HUMIDITY_AVG'].mean() if 'HUMIDITY_AVG' in df_original.columns else 75.0
        weather_forecast = pd.DataFrame({
            'DATE_ONLY': [start_forecast + timedelta(days=i) for i in range(num_days)],
            'TEMPERATURE_AVG': [temp_avg_mean] * num_days,
            'TEMPERATURE_MAX': [temp_avg_mean + 3] * num_days,
            'HUMIDITY_AVG': [humidity_avg_mean] * num_days
        })
    
    # Chu·∫©n h√≥a DATE_ONLY trong weather_forecast v·ªÅ date object
    weather_forecast['DATE_ONLY'] = weather_forecast['DATE_ONLY'].apply(
        lambda x: x if isinstance(x, type(start_forecast)) else pd.to_datetime(x).date() if pd.notna(pd.to_datetime(x, errors='coerce')) else None
    )
    
    # Kh·ªüi t·∫°o window t·ª´ d·ªØ li·ªáu hi·ªán t·∫°i
    current_window = df_processed.iloc[-timesteps:].copy()
    predictions = []
    forecast_dates = []
    
    ts = TrainingService()
    
    for day_idx in range(num_days):
        forecast_date = start_forecast + timedelta(days=day_idx)
        forecast_dates.append(forecast_date.strftime('%Y-%m-%d'))
        
        # L·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt cho ng√†y n√†y
        weather_row = weather_forecast[weather_forecast['DATE_ONLY'] == forecast_date]
        if weather_row.empty:
            # N·∫øu kh√¥ng c√≥, d√πng gi√° tr·ªã trung b√¨nh
            temp_avg = df_original['TEMPERATURE_AVG'].mean() if 'TEMPERATURE_AVG' in df_original.columns else 28.0
            humidity_avg = df_original['HUMIDITY_AVG'].mean() if 'HUMIDITY_AVG' in df_original.columns else 75.0
        else:
            temp_avg = float(weather_row.iloc[0]['TEMPERATURE_AVG']) if pd.notna(weather_row.iloc[0].get('TEMPERATURE_AVG')) else 28.0
            humidity_avg = float(weather_row.iloc[0]['HUMIDITY_AVG']) if pd.notna(weather_row.iloc[0].get('HUMIDITY_AVG')) else 75.0
        
        # T·∫°o row m·ªõi v·ªõi features cho ng√†y d·ª± b√°o
        new_row_data = {
            'TEMPERATURE_AVG': temp_avg,
            'HUMIDITY_AVG': humidity_avg,
        }
        
        # T√≠nh calendar features
        try:
            import holidays
            vn_holidays = holidays.country_holidays('VN')
            new_row_data['HOLIDAY'] = 1 if forecast_date in vn_holidays else 0
        except Exception:
            new_row_data['HOLIDAY'] = 0
        
        month = forecast_date.month
        weekday = forecast_date.weekday()  # Monday=0, Sunday=6
        new_row_data['month_sin'] = np.sin(2 * np.pi * month / 12)
        new_row_data['month_cos'] = np.cos(2 * np.pi * month / 12)
        new_row_data['weekday_sin'] = np.sin(2 * np.pi * weekday / 7)
        new_row_data['weekday_cos'] = np.cos(2 * np.pi * weekday / 7)
        
        # T·∫°o row m·ªõi v·ªõi ƒë·∫ßy ƒë·ªß features theo th·ª© t·ª± c·ªßa df_processed
        new_row_dict = {}
        for col in df_processed.columns:
            if col == 'ENERGY_ADJ' or col == 'ENERGY':
                new_row_dict[col] = 0.0  # T·∫°m th·ªùi, s·∫Ω ƒë∆∞·ª£c thay th·∫ø sau
            elif col in new_row_data:
                new_row_dict[col] = new_row_data[col]
            else:
                # N·∫øu thi·∫øu feature, d√πng gi√° tr·ªã trung b√¨nh t·ª´ window hi·ªán t·∫°i
                new_row_dict[col] = current_window[col].mean() if col in current_window.columns else 0.0
        
        # T·∫°o DataFrame v·ªõi ƒë√∫ng th·ª© t·ª± c·ªôt
        new_row_df = pd.DataFrame([new_row_dict], columns=df_processed.columns)
        
        # D·ª± b√°o: scale window hi·ªán t·∫°i v√† d·ª± b√°o
        window_scaled = scaler.transform(current_window.values)
        x_in = np.expand_dims(window_scaled, axis=0)
        y_hat_scaled = model.predict(x_in, verbose=0)
        n_feat = window_scaled.shape[1] - 1
        dummy = np.zeros((1, n_feat + 1))
        dummy[0, -1] = y_hat_scaled[0, 0]
        y_hat = scaler.inverse_transform(dummy)[0, -1]
        predictions.append(float(y_hat))
        
        # C·∫≠p nh·∫≠t window: th√™m row m·ªõi v·ªõi gi√° tr·ªã d·ª± b√°o
        new_row_df['ENERGY_ADJ'] = y_hat
        # C·∫≠p nh·∫≠t window: b·ªè row ƒë·∫ßu, th√™m row m·ªõi (gi·ªØ ·ªü d·∫°ng unprocessed)
        current_window = pd.concat([
            current_window.iloc[1:],
            new_row_df
        ], ignore_index=True)
    
    return forecast_dates, predictions

@app.get("/api/data/forecast/week")
async def get_forecast_week(user: EvnAccount = Depends(get_user_by_username), db: Session = Depends(get_db)):
    """D·ª± b√°o cho 7 ng√†y ti·∫øp theo"""
    import joblib
    import numpy as np
    from server.services.training_service import TrainingService
    from server.database import get_account_by_username
    model_dir = MODELS_DIR / f"user_{user.evn_username}"
    model_path = model_dir / "lstm_model.h5"
    sx_path = model_dir / "scaler_x.pkl"
    sy_path = model_dir / "scaler_y.pkl"
    if not (model_path.exists() and sx_path.exists() and sy_path.exists()):
        raise HTTPException(status_code=404, detail="Model ch∆∞a s·∫µn s√†ng")
    acc = get_account_by_username(db, user.evn_username)
    if not acc:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y t√†i kho·∫£n")
    location = acc.location if acc.location else "Ho Chi Minh City"
    import tensorflow as tf
    model = tf.keras.models.load_model(model_path, compile=False)
    timesteps = 7
    scaler = joblib.load(sx_path)
    ts = TrainingService()
    df = ts.build_dataset_from_db(db, user.evn_username, location)
    if df.empty:
        raise HTTPException(status_code=404, detail="Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o")
    df_processed = ts.preprocess_for_base_model(df)
    if len(df_processed) < timesteps + 1:
        raise HTTPException(status_code=400, detail=f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o. C·∫ßn √≠t nh·∫•t {timesteps + 1} m·∫´u")
    
    try:
        forecast_dates, predictions = forecast_multiple_days(
            model, scaler, df_processed, df, timesteps, 7, location, db
        )
        return {
            "horizon": 7,
            "unit": "days",
            "forecast_dates": forecast_dates,
            "predictions": predictions
        }
    except Exception as e:
        logger.error(f"L·ªói khi d·ª± b√°o 7 ng√†y: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"L·ªói khi d·ª± b√°o: {str(e)}")

@app.get("/api/data/forecast/month")
async def get_forecast_month(user: EvnAccount = Depends(get_user_by_username), db: Session = Depends(get_db)):
    """D·ª± b√°o cho 30 ng√†y ti·∫øp theo"""
    import joblib
    import numpy as np
    from server.services.training_service import TrainingService
    from server.database import get_account_by_username
    model_dir = MODELS_DIR / f"user_{user.evn_username}"
    model_path = model_dir / "lstm_model.h5"
    sx_path = model_dir / "scaler_x.pkl"
    sy_path = model_dir / "scaler_y.pkl"
    if not (model_path.exists() and sx_path.exists() and sy_path.exists()):
        raise HTTPException(status_code=404, detail="Model ch∆∞a s·∫µn s√†ng")
    acc = get_account_by_username(db, user.evn_username)
    if not acc:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y t√†i kho·∫£n")
    location = acc.location if acc.location else "Ho Chi Minh City"
    import tensorflow as tf
    model = tf.keras.models.load_model(model_path, compile=False)
    timesteps = 7
    scaler = joblib.load(sx_path)
    ts = TrainingService()
    df = ts.build_dataset_from_db(db, user.evn_username, location)
    if df.empty:
        raise HTTPException(status_code=404, detail="Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o")
    df_processed = ts.preprocess_for_base_model(df)
    if len(df_processed) < timesteps + 1:
        raise HTTPException(status_code=400, detail=f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ d·ª± b√°o. C·∫ßn √≠t nh·∫•t {timesteps + 1} m·∫´u")
    
    try:
        forecast_dates, predictions = forecast_multiple_days(
            model, scaler, df_processed, df, timesteps, 30, location, db
        )
        return {
            "horizon": 30,
            "unit": "days",
            "forecast_dates": forecast_dates,
            "predictions": predictions
        }
    except Exception as e:
        logger.error(f"L·ªói khi d·ª± b√°o 30 ng√†y: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"L·ªói khi d·ª± b√°o: {str(e)}")

# History from MySQL (paged)
@app.get("/api/data/history/db")
async def get_history_db(
    user: EvnAccount = Depends(get_user_by_username),
    db: Session = Depends(get_db),
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    page: int = 1,
    page_size: int = 100
):
    from datetime import datetime as dt
    q = db.query(DailyConsumption).filter(DailyConsumption.evn_username == user.evn_username)
    try:
        if start_date:
            sd = dt.fromisoformat(start_date).date()
            q = q.filter(DailyConsumption.date >= sd)
        if end_date:
            ed = dt.fromisoformat(end_date).date()
            q = q.filter(DailyConsumption.date <= ed)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD")
    total = q.count()
    page = max(1, page)
    page_size = max(1, min(page_size, 1000))
    rows = q.order_by(DailyConsumption.date.asc()).offset((page - 1) * page_size).limit(page_size).all()
    items = [
        {
            "date": r.date.isoformat() if r.date else None,
            "consumption_kwh": float(r.consumption_kwh) if r.consumption_kwh is not None else None
        }
        for r in rows
    ]
    return {
        "total": total,
        "page": page,
        "page_size": page_size,
        "items": items
    }
    

from pydantic import BaseModel

class ChatRequest(BaseModel):
    question: str

@app.post("/api/chat")
def chat_with_ai(
    evn_username: str,
    req: ChatRequest,
    db: Session = Depends(get_db)
):
    answer = ask_energy_ai_for_user(
        evn_username=evn_username,
        question=req.question,
        db=db,
        forecast_horizon=30
    )
    return {
        "question": req.question,
        "answer": answer
    }




if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

